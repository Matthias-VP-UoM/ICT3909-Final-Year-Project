{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mvass\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pytubefix\n",
    "import json\n",
    "import pickle\n",
    "import cv2\n",
    "import time\n",
    "import datetime\n",
    "import shutil   # ONLY USED FOR CLEARING EXISTING FRAMES\n",
    "from common_functionality import clear_folder\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "from translate import Translator\n",
    "from googletrans import Translator as GoogleTranslator\n",
    "from langdetect import detect\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interview Video Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video website functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(dir_name):\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_json_file(file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(json.dumps({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_processed_file(file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(json.dumps({'processed_urls': []}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_contents_pkl(file_path, data):\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_videos(processed_file):\n",
    "    with open(processed_file) as f:\n",
    "        processed_data = json.load(f)\n",
    "\n",
    "        if 'processed_urls' not in processed_data:\n",
    "            init_processed_file(processed_file)\n",
    "\n",
    "        return processed_data['processed_urls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_processed_videos(url, processed_file, processed_file_pkl):\n",
    "    with open(processed_file, 'r+') as f:\n",
    "        processed_data = json.load(f)\n",
    "        if 'processed_urls' not in processed_data.keys():\n",
    "            processed_data['processed_urls'] = []\n",
    "        \n",
    "        if url not in processed_data['processed_urls']:\n",
    "            processed_data['processed_urls'].append(url)\n",
    "        f.seek(0)\n",
    "        json.dump(processed_data, f, indent=4)\n",
    "        save_contents_pkl(processed_file_pkl, processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_from_file(file_name):\n",
    "    with open(file_name) as f:\n",
    "        data = json.load(f)\n",
    "        return data['episode_urls']\n",
    "\n",
    "def get_processed_url_from_file(file_name):\n",
    "    with open(file_name) as f:\n",
    "        data = json.load(f)\n",
    "        return data['processed_urls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_from_file_pkl(file_name):\n",
    "    with open(file_name) as f:\n",
    "        data = pickle.load(file_name)\n",
    "        return data['episode_urls']\n",
    "\n",
    "def get_processed_url_from_file_pkl(file_name):\n",
    "    with open(file_name) as f:\n",
    "        data = pickle.load(file_name)\n",
    "        return data['processed_urls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_video_from_yt(videos_dir, url, processed_file):\n",
    "    num_videos = len([f for f in os.listdir(videos_dir) if os.path.isfile(os.path.join(videos_dir, f))])\n",
    "\n",
    "    finished_video_urls = get_processed_videos(processed_file)\n",
    "\n",
    "    if url not in finished_video_urls:\n",
    "        try:\n",
    "            yt = pytubefix.YouTube(url)\n",
    "        except:\n",
    "            print(\"Connection error!\")\n",
    "\n",
    "        video = yt.streams.get_highest_resolution()\n",
    "\n",
    "        try:\n",
    "            print('Downloading video', video.title)\n",
    "            video.download(videos_dir, f'video_{num_videos+1}.mp4')\n",
    "            print(f\"Youtube video \\'{video.title}\\' has been successfully downloaded!\")\n",
    "        except:\n",
    "            print(\"Download error!\")\n",
    "    else:\n",
    "        print(f\"Video with following url is already in local storage in the directory \\'{videos_dir}\\': {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(videos_dir, urls_file, processed_file, processed_file_pkl, data_dir):\n",
    "    create_dir(videos_dir)\n",
    "    create_dir(data_dir)\n",
    "\n",
    "    url_path = data_dir+'/'+urls_file\n",
    "    processed_path = data_dir+'/'+processed_file\n",
    "    processed_path_pkl = data_dir+'/'+processed_file_pkl\n",
    "\n",
    "    if not os.path.isfile(processed_path):\n",
    "        create_json_file(processed_path)\n",
    "\n",
    "    urls = get_url_from_file(url_path)\n",
    "\n",
    "    print(urls)\n",
    "\n",
    "    for index, url in enumerate(urls):\n",
    "        download_video_from_yt(videos_dir, url, processed_path)\n",
    "        set_processed_videos(url, processed_path, processed_path_pkl)\n",
    "\n",
    "    print(f\"Downloaded videos can be found in the diretory \\\"{videos_dir}\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.youtube.com/watch?v=qyxkN-xr7tw', 'https://www.youtube.com/watch?v=yKPHOhfMBpE', 'https://www.youtube.com/watch?v=g6_4k0_Dpho', 'https://www.youtube.com/watch?v=WvC4srvFIm4', 'https://www.youtube.com/watch?v=bpwP-m9x-U0', 'https://www.youtube.com/watch?v=0Agt9eA6KK8', 'https://www.youtube.com/watch?v=hGvouXV27MA', 'https://www.youtube.com/watch?v=WbJoyLEveIA', 'https://www.youtube.com/watch?v=VxGUnKG78Eg', 'https://www.youtube.com/watch?v=u3e2Ubtua98', 'https://www.youtube.com/watch?v=TqMcSBy6gaw', 'https://www.youtube.com/watch?v=KF60nsc9Stg', 'https://www.youtube.com/watch?v=GeTsMrMqmCY', 'https://www.youtube.com/watch?v=sd7igZiKJQM', 'https://www.youtube.com/watch?v=wZcCBdAWXgE', 'https://www.youtube.com/watch?v=z8e8StV8cNk', 'https://www.youtube.com/watch?v=_RKje9yVjy8', 'https://www.youtube.com/watch?v=Vg346770llI', 'https://www.youtube.com/watch?v=RQOx3OXTZVc', 'https://www.youtube.com/watch?v=l5arBrIVqYA', 'https://www.youtube.com/watch?v=DAvTAM1JIaA', 'https://www.youtube.com/watch?v=vvDRq84R65E']\n",
      "Video with following url is already in local storage in the directory 'videos_db': https://www.youtube.com/watch?v=qyxkN-xr7tw\n",
      "Video with following url is already in local storage in the directory 'videos_db': https://www.youtube.com/watch?v=yKPHOhfMBpE\n",
      "Video with following url is already in local storage in the directory 'videos_db': https://www.youtube.com/watch?v=g6_4k0_Dpho\n",
      "Video with following url is already in local storage in the directory 'videos_db': https://www.youtube.com/watch?v=WvC4srvFIm4\n",
      "Video with following url is already in local storage in the directory 'videos_db': https://www.youtube.com/watch?v=bpwP-m9x-U0\n",
      "Video with following url is already in local storage in the directory 'videos_db': https://www.youtube.com/watch?v=0Agt9eA6KK8\n",
      "Video with following url is already in local storage in the directory 'videos_db': https://www.youtube.com/watch?v=hGvouXV27MA\n",
      "Video with following url is already in local storage in the directory 'videos_db': https://www.youtube.com/watch?v=WbJoyLEveIA\n",
      "Video with following url is already in local storage in the directory 'videos_db': https://www.youtube.com/watch?v=VxGUnKG78Eg\n",
      "Video with following url is already in local storage in the directory 'videos_db': https://www.youtube.com/watch?v=u3e2Ubtua98\n",
      "Video with following url is already in local storage in the directory 'videos_db': https://www.youtube.com/watch?v=TqMcSBy6gaw\n",
      "Video with following url is already in local storage in the directory 'videos_db': https://www.youtube.com/watch?v=KF60nsc9Stg\n",
      "Video with following url is already in local storage in the directory 'videos_db': https://www.youtube.com/watch?v=GeTsMrMqmCY\n",
      "Video with following url is already in local storage in the directory 'videos_db': https://www.youtube.com/watch?v=sd7igZiKJQM\n",
      "Video with following url is already in local storage in the directory 'videos_db': https://www.youtube.com/watch?v=wZcCBdAWXgE\n",
      "Video with following url is already in local storage in the directory 'videos_db': https://www.youtube.com/watch?v=z8e8StV8cNk\n",
      "Video with following url is already in local storage in the directory 'videos_db': https://www.youtube.com/watch?v=_RKje9yVjy8\n",
      "Video with following url is already in local storage in the directory 'videos_db': https://www.youtube.com/watch?v=Vg346770llI\n",
      "Video with following url is already in local storage in the directory 'videos_db': https://www.youtube.com/watch?v=RQOx3OXTZVc\n",
      "Video with following url is already in local storage in the directory 'videos_db': https://www.youtube.com/watch?v=l5arBrIVqYA\n",
      "Video with following url is already in local storage in the directory 'videos_db': https://www.youtube.com/watch?v=DAvTAM1JIaA\n",
      "Video with following url is already in local storage in the directory 'videos_db': https://www.youtube.com/watch?v=vvDRq84R65E\n",
      "Downloaded videos can be found in the diretory \"videos_db\"\n"
     ]
    }
   ],
   "source": [
    "urls_list = 'urls_list.json'\n",
    "processed_urls_list = 'processed_urls_list.json'\n",
    "processed_urls_list_pkl = 'processed_urls_list.pkl'\n",
    "videos_dir = 'videos_db'\n",
    "data_dir = 'data'\n",
    "\n",
    "main(videos_dir, urls_list, processed_urls_list, processed_urls_list_pkl, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.youtube.com/watch?v=EJ1dlym0KI8', 'https://www.youtube.com/watch?v=NjwqPOgz3pc']\n",
      "Video with following url is already in local storage in the directory 'videos_db_ml': https://www.youtube.com/watch?v=EJ1dlym0KI8\n",
      "Video with following url is already in local storage in the directory 'videos_db_ml': https://www.youtube.com/watch?v=NjwqPOgz3pc\n",
      "Downloaded videos can be found in the diretory \"videos_db_ml\"\n"
     ]
    }
   ],
   "source": [
    "#NOTE: ONLY TO BE USED TO PROCESS THE VIDEOS DONE BY MARK LAWRENCE\n",
    "urls_list_ml = 'urls_list_ml.json'\n",
    "processed_urls_list_ml = 'processed_urls_list_ml.json'\n",
    "processed_urls_list_pkl_ml = 'processed_urls_list_ml.pkl'\n",
    "ml_videos_dir = 'videos_db_ml'\n",
    "data_dir = 'data'\n",
    "\n",
    "main(ml_videos_dir, urls_list_ml, processed_urls_list_ml, processed_urls_list_pkl_ml, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_json_file(file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(json.dumps({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_json_content(file_path, pic_dict):\n",
    "    with open(file_path, 'r+') as f:\n",
    "        processed_data = json.load(f)\n",
    "        if 'frames_list' not in processed_data.keys():\n",
    "            processed_data['frames_list'] = []\n",
    "        \n",
    "        processed_data['frames_list'].append(pic_dict)\n",
    "        f.seek(0)\n",
    "        json.dump(processed_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_video_date(url, file_path):\n",
    "    try:\n",
    "        yt = pytubefix.YouTube(url)\n",
    "    except:\n",
    "        print(\"Connection error!\")\n",
    "    \n",
    "    video_date = yt.publish_date.date()\n",
    "\n",
    "    date_format = \"%d-%m-%Y\"\n",
    "\n",
    "    video_date = video_date.strftime(date_format)\n",
    "\n",
    "    print(str(video_date))\n",
    "    \n",
    "    with open(file_path, 'r+') as f:\n",
    "        processed_data = json.load(f)\n",
    "        #print(not bool(processed_data))\n",
    "        if 'video_date' not in processed_data.keys():\n",
    "            if bool(processed_data):\n",
    "                processed_data_ordered = OrderedDict(processed_data)\n",
    "                # Make a list of key-value pairs\n",
    "                items = list(processed_data_ordered.items())\n",
    "\n",
    "                # Find where 'frames_list' is\n",
    "                index = next(i for i, (k, v) in enumerate(items) if k == \"frames_list\")\n",
    "\n",
    "                # Insert new key-value pair before it\n",
    "                items.insert(index, ('video_date', str(video_date)))\n",
    "\n",
    "                # Rebuild the OrderedDict\n",
    "                processed_data_ordered = OrderedDict(items)\n",
    "\n",
    "                processed_data = dict(processed_data_ordered)\n",
    "            else:\n",
    "                processed_data['video_date'] = str(video_date)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        print(processed_data)\n",
    "        \n",
    "        f.seek(0)\n",
    "        json.dump(processed_data, f, indent=4)\n",
    "        f.truncate()  # This prevents leftover characters from previous write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19-10-2023\n",
      "{'video_date': '19-10-2023'}\n",
      "09-02-2023\n",
      "{'video_date': '09-02-2023'}\n",
      "02-02-2023\n",
      "{'video_date': '02-02-2023'}\n",
      "04-07-2024\n",
      "{'video_date': '04-07-2024'}\n",
      "25-04-2024\n",
      "{'video_date': '25-04-2024'}\n",
      "01-06-2023\n",
      "{'video_date': '01-06-2023'}\n",
      "12-01-2023\n",
      "{'video_date': '12-01-2023'}\n",
      "22-09-2022\n",
      "{'video_date': '22-09-2022'}\n",
      "08-12-2022\n",
      "{'video_date': '08-12-2022'}\n",
      "06-04-2023\n",
      "{'video_date': '06-04-2023'}\n",
      "29-08-2024\n",
      "{'video_date': '29-08-2024'}\n",
      "22-08-2024\n",
      "{'video_date': '22-08-2024'}\n",
      "16-05-2024\n",
      "{'video_date': '16-05-2024'}\n",
      "07-03-2024\n",
      "{'video_date': '07-03-2024'}\n",
      "08-02-2024\n",
      "{'video_date': '08-02-2024'}\n",
      "21-12-2023\n",
      "{'video_date': '21-12-2023'}\n",
      "09-11-2023\n",
      "{'video_date': '09-11-2023'}\n",
      "29-07-2021\n",
      "{'video_date': '29-07-2021'}\n",
      "07-11-2024\n",
      "{'video_date': '07-11-2024'}\n",
      "19-12-2024\n",
      "{'video_date': '19-12-2024'}\n",
      "21-11-2024\n",
      "{'video_date': '21-11-2024'}\n",
      "29-12-2024\n",
      "{'video_date': '29-12-2024'}\n"
     ]
    }
   ],
   "source": [
    "videos_dir = 'videos_db'\n",
    "imgs_dir = 'vid_frames'\n",
    "\n",
    "data_folder = 'data'\n",
    "video_data_folder = 'video_data'\n",
    "full_vid_data_path = os.path.join(data_folder, video_data_folder)\n",
    "\n",
    "if not os.path.exists(full_vid_data_path):\n",
    "    os.makedirs(full_vid_data_path)\n",
    "\n",
    "processed_path = os.path.join(data_dir, processed_urls_list)\n",
    "urls = get_processed_url_from_file(processed_path)\n",
    "\n",
    "for url_index, url in enumerate(urls):\n",
    "    full_data_file_path = os.path.join(full_vid_data_path, f'video_{url_index+1}.json')\n",
    "    if not os.path.exists(full_data_file_path):\n",
    "        create_json_file(full_data_file_path)\n",
    "    add_video_date(url, full_data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26-02-2025\n",
      "{'video_date': '26-02-2025'}\n",
      "12-03-2025\n",
      "{'video_date': '12-03-2025'}\n"
     ]
    }
   ],
   "source": [
    "#NOTE: ONLY TO BE USED TO PROCESS THE VIDEOS DONE BY MARK LAWRENCE\n",
    "videos_dir = 'videos_db_ml'\n",
    "imgs_dir = 'vid_frames_ml'\n",
    "\n",
    "data_folder = 'data'\n",
    "video_data_folder = 'video_data_ml'\n",
    "full_vid_data_path = os.path.join(data_folder, video_data_folder)\n",
    "\n",
    "if not os.path.exists(full_vid_data_path):\n",
    "    os.mkdir(full_vid_data_path)\n",
    "\n",
    "processed_path = os.path.join(data_dir, processed_urls_list_ml)\n",
    "urls = get_processed_url_from_file(processed_path)\n",
    "\n",
    "for url_index, url in enumerate(urls):\n",
    "    full_data_file_path = os.path.join(full_vid_data_path, f'video_{url_index+1}.json')\n",
    "    if not os.path.exists(full_data_file_path):\n",
    "        create_json_file(full_data_file_path)\n",
    "    add_video_date(url, full_data_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interview Name Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spaCy English model\n",
    "nlp = spacy.load('en_core_web_trf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "interviewee_data = 'interviewee_names_data.json'\n",
    "interviewer_data = 'interviewer_names_data.json'\n",
    "processed_data = {}\n",
    "interviewees_by_video = {}\n",
    "interviewers_by_video = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: ONLY TO BE USED TO PROCESS THE VIDEOS DONE BY MARK LAWRENCE\n",
    "interviewee_data = 'interviewee_names_data_ml.json'\n",
    "interviewer_data = 'interviewer_names_data_ml.json'\n",
    "processed_data = {}\n",
    "interviewees_by_video = {}\n",
    "interviewers_by_video = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(data_folder):\n",
    "    if os.path.exists(os.path.join(data_folder, processed_urls_list)):\n",
    "        with open(os.path.join(data_folder, processed_urls_list), 'r') as f:\n",
    "            processed_data = json.load(f)['processed_urls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: ONLY TO BE USED TO PROCESS THE VIDEOS DONE BY MARK LAWRENCE\n",
    "if os.path.exists(data_folder):\n",
    "    if os.path.exists(os.path.join(data_folder, processed_urls_list_ml)):\n",
    "        with open(os.path.join(data_folder, processed_urls_list_ml), 'r') as f:\n",
    "            processed_data = json.load(f)['processed_urls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_name_from_text(text):\n",
    "    doc = nlp(text)\n",
    "    detected_names = []\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in ['PERSON', 'ORG']:  # Check for misclassified names\n",
    "            # Check if it's a misclassified person\n",
    "            #if ent.label_ == \"ORG\" and len(ent.text.split()) > 1:  \n",
    "                #print(f\"Correcting {ent.text} from ORG to PERSON\")\n",
    "                #detected_names.append(ent.text)\n",
    "            if ent.label_ == \"PERSON\":\n",
    "                detected_names.append(ent.text)\n",
    "\n",
    "    if detected_names:\n",
    "        interviewer = \" \".join(detected_names)  # Handle partial name detection\n",
    "        #persons_list.append(full_name)\n",
    "        #interviewees_by_video[f'video_{index+1}'] = full_name\n",
    "        print(interviewer, \"- PERSON\")\n",
    "    else:\n",
    "        print(\"No names have been detected from the video title!\")\n",
    "        return \"\"\n",
    "    \n",
    "    return interviewer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise translators\n",
    "translator = Translator(to_lang=\"en\", from_lang=\"mt\")\n",
    "google_translator = GoogleTranslator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to check the language of the video title\n",
    "def detect_title_language(text):\n",
    "    try:\n",
    "        detected_language = detect(text)\n",
    "        return detected_language\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translation using the translate package\n",
    "def translate_text_to_english(text):\n",
    "    text_en = translator.translate(text)\n",
    "    return text_en\n",
    "\n",
    "# Translation using the Google Translate API\n",
    "async def translate_text_to_english_google(text):\n",
    "    text_en = await google_translator.translate(text, dest=\"en\")\n",
    "    text_en = text_en.text\n",
    "    return text_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example first names and surnames lists\n",
    "first_names = [\"jon\", \"mark\", \"dylan\", \"chris\", \"ian\", \"michael\", \"matthew\", \"laurence\", \"joseph\", \"andrew\"]\n",
    "surnames = [\"mallia\", \"zammit\", \"fearne\", \"borg\", \"schembri\", \"fenech\", \"galea\", \"camilleri\"]\n",
    "\n",
    "def smart_name_split(name_raw):\n",
    "    name_raw = name_raw.lower()\n",
    "\n",
    "    # Try all possible split points\n",
    "    for i in range(2, len(name_raw) - 2):  # avoid splitting too early or too late\n",
    "        first = name_raw[:i]\n",
    "        last = name_raw[i:]\n",
    "        if first in first_names and last in surnames:\n",
    "            return f\"{first.capitalize()} {last.capitalize()}\"\n",
    "\n",
    "    # fallback: just capitalize first letter\n",
    "    return name_raw.capitalize()\n",
    "\n",
    "def extract_interviewer(description):\n",
    "    patterns = [\n",
    "        r\"interview with ([A-Z][a-z]+(?: [A-Z][a-z]+){0,2})(?: and|,|\\.|$)\",\n",
    "        r\"produced by ([A-Z][a-z]+(?: [A-Z][a-z]+){0,2})(?: and|,|\\.|$)\",\n",
    "        r\"hosted by ([A-Z][a-z]+(?: [A-Z][a-z]+){0,2})(?: and|,|\\.|$)\",\n",
    "        r\"presented by ([A-Z][a-z]+(?: [A-Z][a-z]+){0,2})(?: and|,|\\.|$)\",\n",
    "        r\"moderated by ([A-Z][a-z]+(?: [A-Z][a-z]+){0,2})(?: and|,|\\.|$)\",\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, description, re.IGNORECASE)\n",
    "        if match:\n",
    "            name = match.group(1).strip()\n",
    "            return name\n",
    "\n",
    "    # Fallback to website domain\n",
    "    # website_match = re.search(r\"https?://(?:www\\.)?([a-z]+)\\.mt\", description)\n",
    "    # if website_match:\n",
    "    #     name_raw = website_match.group(1)\n",
    "    #     return smart_name_split(name_raw)\n",
    "\n",
    "    # Fallback to hashtag\n",
    "    hashtag_match = re.search(r\"#([a-z]+)\", description)\n",
    "    if hashtag_match:\n",
    "        name_raw = hashtag_match.group(1)\n",
    "        return smart_name_split(name_raw)\n",
    "\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Author of video_1: Times of Malta\n",
      "No names have been detected from the video title!\n",
      "Mark Laurence Zammit\n",
      "------------------------------------------------------------\n",
      "Detected Author of video_2: Times of Malta\n",
      "No names have been detected from the video title!\n",
      "Mark Laurence Zammit\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "phrases_to_remove_videos = [['Podcast'] for i in processed_data]\n",
    "print(len(phrases_to_remove_videos))\n",
    "words_to_ignore = ['Episode', 'with', 'Part']\n",
    "\n",
    "interviewers_list = []\n",
    "for index, url in enumerate(processed_data):\n",
    "    yt = pytubefix.YouTube(url)\n",
    "    print(f'Detected Author of video_{index+1}: {yt.author}')\n",
    "\n",
    "    detected_language = detect_title_language(yt.author)\n",
    "    video_author_en = \"\"\n",
    "    video_description_en = \"\"\n",
    "\n",
    "    # Translate title to English\n",
    "    if detected_language != 'en':\n",
    "        video_author_en = translate_text_to_english(yt.author)\n",
    "        #video_title_en = GoogleTranslator(source='mt', target='en').translate(yt.title)\n",
    "        print('Video Title Name translated to English:', video_author_en)\n",
    "    else:\n",
    "        video_author_en = yt.author\n",
    "\n",
    "    # Process the text with spaCy\n",
    "    detected_names = []\n",
    "\n",
    "    detected_interviewer = detect_name_from_text(yt.author)\n",
    "    if detected_interviewer == '':\n",
    "        #print(yt.description)\n",
    "        detected_desc_language = detect_title_language(yt.description)\n",
    "        if detected_language != 'en':\n",
    "            #video_description_en = translate_text_to_english(yt.description)\n",
    "            video_description_en = translate_text_to_english_google(yt.description)\n",
    "            print(video_description_en)\n",
    "        else:\n",
    "            video_description_en = yt.description\n",
    "        detected_interviewer = extract_interviewer(video_description_en)\n",
    "\n",
    "    print('No name' if detected_interviewer == '' else detected_interviewer)\n",
    "    interviewers_list.append(detected_interviewer)\n",
    "    interviewers_by_video[f'video_{index+1}'] = detected_interviewer\n",
    "    phrases_to_remove_videos[index].extend(word for word in list(detected_interviewer.split()))\n",
    "    \n",
    "    print('------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Title Name of video_1: Ian Borg, the man with many plans\n",
      "Ian Borg, the man with many plans\n",
      "Ian Borg - PERSON\n",
      "------------------------------------------------------------\n",
      "Detected Title Name of video_2: What a pandemic taught Chris Fearne\n",
      "What a pandemic taught Chris Fearne\n",
      "Chris Fearne - PERSON\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "persons_list = []\n",
    "for index, url in enumerate(processed_data):\n",
    "    yt = pytubefix.YouTube(url)\n",
    "    print(f'Detected Title Name of video_{index+1}: {yt.title}')\n",
    "\n",
    "    detected_language = detect_title_language(yt.title)\n",
    "    video_title_en = \"\"\n",
    "\n",
    "    # Translate title to English\n",
    "    if detected_language != 'en':\n",
    "        video_title_en = translator.translate(yt.title)\n",
    "        #video_title_en = GoogleTranslator(source='mt', target='en').translate(yt.title)\n",
    "        print('Video Title Name translated to English:', video_title_en)\n",
    "    else:\n",
    "        video_title_en = yt.title\n",
    "\n",
    "    # Remove unwanted phrases\n",
    "    title_words = video_title_en.split()\n",
    "    for word in phrases_to_remove_videos[index]:\n",
    "        if word != phrases_to_remove_videos[index][-1]:\n",
    "            if word in title_words:\n",
    "                title_words.remove(word)\n",
    "        else:\n",
    "            pass\n",
    "            #if phrases_to_remove_videos[index][1] in title_words:\n",
    "                #title_words.remove(word)\n",
    "    \n",
    "    #video_title_mod = ' '.join(title_words).replace(\"|\", \"\").strip()\n",
    "    video_title_mod = ' '.join(title_words).split('|')[0]\n",
    "    print(video_title_mod)\n",
    "\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(video_title_mod)\n",
    "    detected_names = []\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in ['PERSON', 'ORG']:  # Check for misclassified names\n",
    "            # Check if it's a misclassified person\n",
    "            #if ent.label_ == \"ORG\" and len(ent.text.split()) > 1:  \n",
    "                #print(f\"Correcting {ent.text} from ORG to PERSON\")\n",
    "                #detected_names.append(ent.text)\n",
    "            if ent.label_ == \"PERSON\":\n",
    "                detected_names.append(ent.text)\n",
    "\n",
    "    if detected_names:\n",
    "        full_name = \" \".join(detected_names)  # Handle partial name detection\n",
    "        persons_list.append(full_name)\n",
    "        interviewees_by_video[f'video_{index+1}'] = full_name\n",
    "        print(full_name, \"- PERSON\")\n",
    "\n",
    "\n",
    "    # ents = list(doc.ents)\n",
    "    # for ent in ents:\n",
    "    #     print(ent, \"-\", ent.label_)\n",
    "    \n",
    "    # for ent in doc.ents:\n",
    "    #     if ent.label_ == 'PERSON' and ent.text not in words_to_ignore:\n",
    "    #         person_name = ent.text\n",
    "    #         persons_list.append(person_name)\n",
    "    #         interviewees_by_video[f'video_{index+1}'] = person_name\n",
    "    #         print(person_name, '-', ent.label_)\n",
    "    \n",
    "    print('------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking if all names have been extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of persons detected: 2\n",
      "All possible names have been detected!\n",
      "{'video_1': 'Mark Laurence Zammit', 'video_2': 'Mark Laurence Zammit'}\n",
      "-----------------------------------------------------------------\n",
      "Number of persons detected: 2\n",
      "All possible names have been detected!\n",
      "{'video_1': 'Ian Borg', 'video_2': 'Chris Fearne'}\n"
     ]
    }
   ],
   "source": [
    "print('Number of persons detected:',len(interviewers_list))\n",
    "\n",
    "if len(interviewers_list) == len(interviewers_list):\n",
    "    print('All possible names have been detected!')\n",
    "\n",
    "print(interviewers_by_video)\n",
    "\n",
    "print('-----------------------------------------------------------------')\n",
    "\n",
    "print('Number of persons detected:',len(persons_list))\n",
    "\n",
    "if len(persons_list) == len(processed_data):\n",
    "    print('All possible names have been detected!')\n",
    "\n",
    "print(interviewees_by_video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving names to data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_folder, interviewer_data), 'w') as file:\n",
    "    json.dump(interviewers_by_video, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_folder, interviewee_data), 'w') as file:\n",
    "    json.dump(interviewees_by_video, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
